# Deep-Learning-Specialization
Deep Learning Specialization consists of 5 courses given by DeepLearning.AI

# About this Specialization
If you want to break into AI, this Specialization will help you do so. Deep Learning is one of the most highly sought after skills in tech. We will help you become good at Deep Learning.

In five courses, you will learn the foundations of Deep Learning, understand how to build neural networks, and learn how to lead successful machine learning projects. You will learn about Convolutional networks, RNNs, LSTM, Adam, Dropout, BatchNorm, Xavier/He initialization, and more. You will work on case studies from healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will master not only the theory, but also see how it is applied in industry. You will practice all these ideas in Python and in TensorFlow, which we will teach.

You will also hear from many top leaders in Deep Learning, who will share with you their personal stories and give you career advice.

AI is transforming multiple industries. After finishing this specialization, you will likely find creative ways to apply it to your work.

We will help you master Deep Learning, understand how to apply it, and build a career in AI.

## Applied Learning Project
You will see and work on case studies in healthcare, autonomous driving, sign language reading, music generation, and natural language processing. You will also build near state-of-the-art deep learning models for several of these applications. In a "Machine Learning flight simulator", you will work through case studies and gain "industry-like experience" setting direction for an ML team.

deeplearning.ai is also partnering with the NVIDIA Deep Learning Institute (DLI) in Course 5, Sequence Models, to provide a programming assignment on Machine Translation with deep learning. You will have the opportunity to build a deep learning project with cutting-edge, industry-relevant content.

# Deep-Learning-Specialization Syllabus
80hs

### 1. Neural Networks and Deep Learning

20 hs

https://www.coursera.org/account/accomplishments/verify/UJYT73TRD8UK

### 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization

18 hs

https://www.coursera.org/account/accomplishments/verify/L5SJGD4EJWWZ

### 3. Structuring Machine Learning Projects

5 hs

https://www.coursera.org/account/accomplishments/verify/N6PU3WJZQ7P9

### 4. Convolutional Neural Networks

20 hs

### 5. Sequence Models -> No llegu√© a anotarme :_(

16 hs


### Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning

Standalone course

30 hs


## 1. Neural Networks and Deep Learning
### WEEK 1
2 hours to complete

Introduction to deep learning

Be able to explain the major trends driving the rise of deep learning, and understand where and how it is applied today.

- Welcome 5m
- What is a neural network? 7m
- Supervised Learning with Neural Networks 8m
- Why is Deep Learning taking off? 10m
- About this Course 2m
- Course Resources 1m
- Geoffrey Hinton interview 40m
- Frequently Asked Questions 10m
- How to use Discussion Forums 10m
- Introduction to deep learning 30m

### WEEK 2
8 hours to complete

Neural Networks Basics

Learn to set up a machine learning problem with a neural network mindset. Learn to use vectorization to speed up your models.

- Binary Classification 8m
- Logistic Regression 5m
- Logistic Regression Cost Function 8m
- Gradient Descent 11m
- Derivatives 7m
- More Derivative Examples 10m
- Computation graph 3m
- Derivatives with a Computation Graph 14m
- Logistic Regression Gradient Descent 6m
- Gradient Descent on m Examples 8m
- Vectorization 8m
- More Vectorization Examples 6m
- Vectorizing Logistic Regression 7m
- Vectorizing Logistic Regression's Gradient Output 9m
- Broadcasting in Python 11m
- A note on python/numpy vectors 6m
- Quick tour of Jupyter/iPython Notebooks 3m
- Explanation of logistic regression cost function (optional) 7m
- Pieter Abbeel interview 16m
- Clarification about Upcoming Logistic Regression Cost Function Video 1m
- Clarification about Upcoming Gradient Descent Video 1m
- Derivation of DL/dz (optional reading) 10m
- Copy of Clarification about Upcoming Logistic Regression Cost Function Video 1m
- Clarification of "dz" 10m
- Deep Learning Honor Code 2m
- Programming Assignment FAQ 10m
- Neural Network Basics 30m

### WEEK 3
5 hours to complete

Shallow neural networks

Learn to build a neural network with one hidden layer, using forward propagation and backpropagation.

- Neural Networks Overview 4m
- Neural Network Representation 5m
- Computing a Neural Network's Output 9m
- Vectorizing across multiple examples 9m
- Explanation for Vectorized Implementation 7m
- Activation functions 10m
- Why do you need non-linear activation functions? 5m
- Derivatives of activation functions 7m
- Gradient descent for Neural Networks 9m
- Backpropagation intuition (optional) 15m
- Random Initialization 7m
- Ian Goodfellow interview 14m
- Clarification: Activation Function 1m
- Clarification about Upcoming Backpropagation intuition (optional) 1m
- Shallow Neural Networks 30m

### WEEK 4
5 hours to complete

Deep Neural Networks

Understand the key computations underlying deep learning, use them to build and train deep neural networks, and apply it to computer vision.

- Deep L-layer neural network 5m
- Forward Propagation in a Deep Network 7m
- Getting your matrix dimensions right 11m
- Why deep representations? 10m
- Building blocks of deep neural networks 8m
- Forward and Backward Propagation 10m
- Parameters vs Hyperparameters 7m
- What does this have to do with the brain? 3m
- Clarification about Getting your matrix dimensions right video 1m
- Clarification about Upcoming Forward and Backward Propagation Video 1m
- Clarification about What does this have to do with the brain video 1m
- Key concepts on Deep Neural Networks 30m

## 2. Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization
### WEEK 1
8 hours to complete

Practical aspects of Deep Learning

- Train / Dev / Test sets 12m
- Bias / Variance 8m
- Basic Recipe for Machine Learning 6m
- Regularization 9m
- Why regularization reduces overfitting? 7m
- Dropout Regularization 9m
- Understanding Dropout 7m
- Other regularization methods 8m
- Normalizing inputs 5m
- Vanishing / Exploding gradients 6m
- Weight Initialization for Deep Networks 6m
- Numerical approximation of gradients 6m
- Gradient checking 6m
- Gradient Checking Implementation Notes 5m
- Yoshua Bengio interview 25m
- Clarification about Upcoming Regularization Video 1m
- Clarification about Upcoming Understanding dropout Video 1m
- Clarification about Upcoming Normalizing Inputs Video 1m
- Practical aspects of deep learning 30m

### WEEK 2
5 hours to complete

Optimization algorithms

- Mini-batch gradient descent 11m
- Understanding mini-batch gradient descent 11m
- Exponentially weighted averages 5m
- Understanding exponentially weighted averages 9m
- Bias correction in exponentially weighted averages 4m
- Gradient descent with momentum 9m
- RMSprop 7m
- Adam optimization algorithm 7m
- Learning rate decay 6m
- The problem of local optima 5m
- Yuanqing Lin interview 13m
- Clarification about Upcoming Adam Optimization Video 1m
- Clarification about Learning Rate Decay Video 1m
- Optimization algorithms 30m

### WEEK 3
5 hours to complete

Hyperparameter tuning, Batch Normalization and Programming Frameworks

- Tuning process 7m
- Using an appropriate scale to pick hyperparameters 8m
- Hyperparameters tuning in practice: Pandas vs. Caviar 6m
- Normalizing activations in a network 8m
- Fitting Batch Norm into a neural network 12m
- Why does Batch Norm work? 11m
- Batch Norm at test time 5m
- Softmax Regression 11m
- Training a softmax classifier 10m
- Deep learning frameworks 4m
- TensorFlow 16m
- Clarifications about Upcoming Softmax Video 1m
- Note about TensorFlow 1 and TensorFlow 2 10m
- Hyperparameter tuning, Batch Normalization, Programming Frameworks 30m

## 3. Structuring Machine Learning Projects
### WEEK 1
2 hours to complete

- Why ML Strategy 2m
- Orthogonalization 10m
- Single number evaluation metric 7m
- Satisficing and Optimizing metric 5m
- Train/dev/test distributions 6m
- Size of the dev and test sets 5m
- When to change dev/test sets and metrics 11m
- Why human-level performance? 5m
- Avoidable bias 6m
- Understanding human-level performance 11m
- Surpassing human-level performance 6m
- Improving your model performance 4m
- Andrej Karpathy interview 15m
- Machine Learning flight simulator 2m
- Bird recognition in the city of Peacetopia (case study) 45m

### WEEK 2
3 hours to complete

- Carrying out error analysis 10m
- Cleaning up incorrectly labeled data 13m
- Build your first system quickly, then iterate 6m
- Training and testing on different distributions 10m
- Bias and Variance with mismatched data distributions 18m
- Addressing data mismatch 10m
- Transfer learning 11m
- Multi-task learning 12m
- What is end-to-end deep learning? 11m
- Whether to use end-to-end deep learning 10m
- Ruslan Salakhutdinov interview 17m
- Autonomous driving (case study) 45m

## 4. Convolutional Neural Networks
### WEEK 1
6 hours to complete

Foundations of Convolutional Neural Networks

Learn to implement the foundational layers of CNNs (pooling, convolutions) and to stack them properly in a deep network to solve multi-class image classification problems.

- Computer Vision 5m
- Edge Detection Example 11m
- More Edge Detection 7m
- Padding 9m
- Strided Convolutions 9m
- Convolutions Over Volume 10m
- One Layer of a Convolutional Network 16m
- Simple Convolutional Network Example 8m
- Pooling Layers 10m
- CNN Example 12m
- Why Convolutions? 9m
- Yann LeCun Interview 27m
- Strided convolutions *CORRECTION* 1m
- Simple Convolutional Network Example *CORRECTION* 1m
- CNN Example *CORRECTION* 1m
- Why Convolutions? *CORRECTION* 1m
- The basics of ConvNets 30m

### WEEK 2
5 hours to complete

Deep convolutional models: case studies

Learn about the practical tricks and methods used in deep CNNs straight from the research papers.

- Why look at case studies? 3m
- Classic Networks 18m
- ResNets 7m
- Why ResNets Work 9m
- Networks in Networks and 1x1 Convolutions 6m
- Inception Network Motivation 10m
- Inception Network 8m
- Using Open-Source Implementation 4m
- Transfer Learning 8m
- Data Augmentation 9m
- State of Computer Vision 12m
- Inception Network Motivation *CORRECTION* 1m
- Deep convolutional models 30m

### WEEK 3
4 hours to complete

Object detection

Learn how to apply your knowledge of CNNs to one of the toughest but hottest field of computer vision: Object detection.

- Object Localization 11m
- Landmark Detection 5m
- Object Detection 5m
- Convolutional Implementation of Sliding Windows 11m
- Bounding Box Predictions 14m
- Intersection Over Union 4m
- Non-max Suppression 8m
- Anchor Boxes 9m
- YOLO Algorithm 7m
- (Optional) Region Proposals 6m
- Convolutional Implementation of Sliding Windows *CORRECTION* 1m
- YOLO algorithm *CORRECTION* 1m
- Detection algorithms 30m

### WEEK 4
5 hours to complete

Special applications: Face recognition & Neural style transfer

Discover how CNNs can be applied to multiple fields, including art generation and face recognition. Implement your own algorithm to generate art and recognize faces!

- What is face recognition? 4m
- One Shot Learning 4m
- Siamese Network 4m
- Triplet Loss 15m
- Face Verification and Binary Classification 6m
- What is neural style transfer? 2m
- What are deep ConvNets learning? 7m
- Cost Function 3m
- Content Cost Function 3m
- Style Cost Function 13m
- 1D and 3D Generalizations 9m
- Triplet Loss *CORRECTION* 1m
- Face Verification and Binary Classification *CORRECTION* 1m
- Style Cost *CORRECTION* 1m
- Special applications: Face recognition & Neural style transfer 30m
